# Anchor Document for AI Agent Development
# Topic: Advanced Testing and Reflection Strategies for CMD Scripts

documentType: "AI_AGENT_TRAINING_ANCHOR"
topic: "Advanced CMD Script Testing & AI Reflection"
version: 1.0
author: "Gemini Agent (based on user feedback)"
date: "2025-10-17"

# Part 1: Evolutionary Testing Tactics for CMD Projects
# A phased approach to building a robust testing culture for CMD scripts.

testingTactics:
  - level: 0
    name: "Manual Baseline"
    description: "The foundation. Manually execute the script, navigate menus, and visually inspect outcomes. The goal is to document key user flows and expected results."
    keywords: ["manual", "baseline", "exploratory"]

  - level: 1
    name: "Basic Assertion Automation"
    description: "The first step into automation. Create simple .cmd test scripts that CALL the main script (or its labels) and check the final %ERRORLEVEL%. Focus on 'happy paths'."
    keywords: ["automation", "errorlevel", "smoke-test", "happy-path"]

  - level: 2
    name: "Output Verification"
    description: "Enhance Level 1 by redirecting script output to a log file (e.g., '> report.log 2>&1'). Use 'findstr' to assert that specific success or error messages are present."
    keywords: ["output", "log", "findstr", "verification"]

  - level: 3
    name: "Advanced Mocking & Side-Effect Testing"
    description: "Test for real-world conditions and verify environmental changes."
    components:
      - name: "Side-Effect Testing"
        details: "Assert that the script had the intended effect on the environment, e.g., 'IF EXIST file.txt', 'reg query HKCU\...', 'sc query ServiceName'."
      - name: "Dependency Mocking"
        details: "For external executables (curl, winget), create mock .cmd versions in a 'tests/mocks' directory. The test script temporarily prepends this directory to the %PATH%, allowing simulation of failure scenarios (e.g., a mock 'curl' that returns a non-zero ERRORLEVEL)."
    keywords: ["mocking", "side-effect", "dependency", "path"]

  - level: 4
    name: "Full CI Integration"
    description: "Integrate all testing levels into a CI pipeline (e.g., GitHub Actions) that runs automatically on a 'windows-latest' runner on every push and pull request."
    keywords: ["ci", "github-actions", "automation"]

# Part 2: Golden Principles for AI-Generated Test Cases
# A set of heuristics for AIs to generate meaningful and robust test cases.

testGenerationPrinciples:
  - principle: "Test the User Story, Not Just the Function"
    description: "Frame tests around user goals. Instead of 'Test the :InstallChrome label', the goal is 'Verify a user can successfully install Chrome non-interactively'. This promotes better E2E and integration tests."
    keywords: ["user-story", "goal-oriented", "e2e"]

  - principle: "The Pessimistic Path Principle"
    description: "Actively try to break the script. Test cases must answer: What if a file is missing? What if the network is down? What if user input is invalid? What if admin rights are missing?"
    keywords: ["negative-testing", "failure-path", "robustness"]

  - principle: "Test for Idempotency"
    description: "If an action is run twice, does it fail or does it gracefully recognize the state? E.g., running an install command for an already-installed app should result in a 'already installed' message, not an error."
    keywords: ["idempotency", "state-aware"]

  - principle: "Test the Boundaries"
    description: "Test with edge-case inputs: empty strings, very long strings, strings with special characters, non-numeric input for numeric menus, etc."
    keywords: ["boundary-testing", "edge-case"]

# Part 3: Three Pillars for AI Self-Reflection and Improvement
# A meta-learning framework for the AI to continuously improve the testing process.

selfReflectionPillars:
  - pillar: 1
    name: "Coverage Analysis"
    reflectionQuestion: "After writing new tests, which functions, menus, options, or parameters in the main script are NOT yet covered by any test case? What is my next priority to close this gap?"
    action: "Programmatically analyze the main script's structure and compare it against the set of existing tests to identify and prioritize coverage gaps."
    keywords: ["coverage", "gap-analysis"]

  - pillar: 2
    name: "Learning from Failures"
    reflectionQuestion: "A CI build failed. Why did the existing test suite not catch this bug before the code was even committed? Do I need a new, more specific test case to prevent this exact class of bug from ever recurring?"
    action: "For every bug fix, a new test case must be created that specifically targets the bug, proving it existed and is now fixed. This is the core of Test-Driven Development (TDD)."
    keywords: ["tdd", "feedback-loop", "regression"]

  - pillar: 3
    name: "Proactive Refactoring for Testability"
    reflectionQuestion: "Before adding this new feature, is the current code structure easy to test? Or should I first perform a small refactor to better isolate logic, making it easier to write a clean unit test for the new feature?"
    action: "Propose small, targeted refactoring efforts with the explicit goal of improving testability and reducing technical debt before implementing new functionality."
    keywords: ["refactoring", "testability", "technical-debt"]
